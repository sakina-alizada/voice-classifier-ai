
# ===============================
# Voice Detection & Categorization
# Data Exploration and Feature Extraction
# ===============================

# Import libraries
import os
import librosa
import librosa.display
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from sklearn.preprocessing import LabelEncoder

# -------------------------------
# 1. Load sample dataset
# -------------------------------

data_dir = '../data/sample_audio/'  # folder with sample audio files
audio_files = [f for f in os.listdir(data_dir) if f.endswith('.wav')]

print(f"Found {len(audio_files)} audio files.")
audio_files[:5]

# -------------------------------
# 2. Audio Exploration
# -------------------------------

# Take first audio file for visualization
sample_file = os.path.join(data_dir, audio_files[0])
y, sr = librosa.load(sample_file, sr=None)

print(f"Audio file: {sample_file}")
print(f"Duration: {librosa.get_duration(y=y, sr=sr):.2f} seconds")
print(f"Sample Rate: {sr}")

# Plot waveform
plt.figure(figsize=(12, 4))
librosa.display.waveshow(y, sr=sr)
plt.title("Waveform")
plt.xlabel("Time (s)")
plt.ylabel("Amplitude")
plt.show()

# Plot spectrogram
X = librosa.stft(y)
Xdb = librosa.amplitude_to_db(abs(X))
plt.figure(figsize=(12, 4))
librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')
plt.colorbar()
plt.title("Spectrogram")
plt.show()

# -------------------------------
# 3. Feature Extraction Function
# -------------------------------

def extract_features(file_path):
    y, sr = librosa.load(file_path, sr=None)
    
    # Extract MFCCs
    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
    mfccs_mean = np.mean(mfccs.T, axis=0)
    
    # Extract Chroma
    stft = np.abs(librosa.stft(y))
    chroma = librosa.feature.chroma_stft(S=stft, sr=sr)
    chroma_mean = np.mean(chroma.T, axis=0)
    
    # Spectral Contrast
    contrast = librosa.feature.spectral_contrast(S=stft, sr=sr)
    contrast_mean = np.mean(contrast.T, axis=0)
    
    # Tonnetz
    y_harmonic = librosa.effects.harmonic(y)
    tonnetz = librosa.feature.tonnetz(y=y_harmonic, sr=sr)
    tonnetz_mean = np.mean(tonnetz.T, axis=0)
    
    # Combine all features
    features = np.concatenate([mfccs_mean, chroma_mean, contrast_mean, tonnetz_mean])
    
    return features

# -------------------------------
# 4. Extract Features for All Files
# -------------------------------

feature_list = []
labels = []

# Assume filenames are like: happy_01.wav, sad_02.wav
for f in audio_files:
    file_path = os.path.join(data_dir, f)
    label = f.split('_')[0]  # get emotion from filename
    features = extract_features(file_path)
    
    feature_list.append(features)
    labels.append(label)

# Convert to DataFrame
X = pd.DataFrame(feature_list)
y = pd.Series(labels, name='label')

print(f"Feature matrix shape: {X.shape}")
print(f"Labels shape: {y.shape}")

# -------------------------------
# 5. Encode Labels
# -------------------------------
le = LabelEncoder()
y_encoded = le.fit_transform(y)
print("Classes:", le.classes_)

# -------------------------------
# 6. Visualize Feature Distributions
# -------------------------------
plt.figure(figsize=(12, 6))
sns.countplot(x=y)
plt.title("Distribution of Emotions in Sample Data")
plt.show()

# Pairplot for first few MFCC features (optional)
feature_df = X.iloc[:, :5].copy()
feature_df['label'] = y
sns.pairplot(feature_df, hue='label')
plt.show()

# -------------------------------
# 7. Save Features for Modeling
# -------------------------------
X['label'] = y
X.to_csv('../data/features.csv', index=False)
print("Features saved to '../data/features.csv'")
